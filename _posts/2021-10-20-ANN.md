---
layout: post
title: Why are neural networks "universal approximators"?
subtitle: From Weierstrass to the Universal Approximation Theorem
cover-img: /assets/img/process1.jpg
thumbnail-img: /assets/img/vision.png
share-img: /assets/img/path.jpg
tags: 
- Machine Learning
- Math
---

Artificial neural networks (ANNs) are originally biologically inspired, and they're very useful tools to model a variety of neural activities, brain functions, and cognitive processes. They are very capable of capturing and describing complex nonlinear dynamics, patterns, and relationships in neural systems, and that is why they find a home in the field of computational neuroscience. 

Although they're simplified models of biological architectures, their power lies in the capacity of approximating a wide range of functions. But how capable are they theoretically, and why are they so powerful? 

In practice, we often encounter functions that don't have straightforward mathematically tractability, especially in some complex biological processes. Many of these processes can be described as continuous functions, so having accesible tools to approximate such functions is crucial. This is where the field of function approximation come into play.

In this article, I will introduce theorems that shed light on approximating continuous functions, ranging from the power of polynomials to neural networks. The first is the Weierstrass approximation theorem, formulated by Karl Weierstrass back 1885. This theorem tells that any continuous function defined on a closed interval can be closely approximated by a polynomial to an arbitrary extent. More than hundred years later (1989), George Cybenko proved the Universal Approximation theorem that expanded our understanding of neural networks' capabilities. The major breakthrough is that it expands the domain from closed intervals to any compact set in higher-dimensional spaces. 

It's worth noting that the theoretical mathematical approximation capabilities of neural networks aren't the primary motivation why they're employed in the context of neuroscience. This is because the theories only provided existence of convergence, but the proven bounds for the size of the ANNs are too loose, thay they aren't a good reference to guide training ANNs in practice. 

these capabilities are indispensable in modeling the complex processes in biological systems. 

---

The Weierstrass approximation theorem

The Weierstrass approximation theorem proves that any continuous function on a closed and bounded interval can be uniformly approximated on that interval by a polynomial to any level of accuracy. 

Let's take a look at the proof in the classical real analysis textbook ***Principles of Mathematical Analysis, by Walter Rudin***.

<div class="postcell post-layout--right">  
  <blockquote><strong>Theorem:</strong><p>If <span class="math-container">$f$</span> is a continuous complex function on <span class="math-container">$[a, b]$</span>, there exists a sequence of polynomials <span class="math-container">$P_n$</span> such that
<span class="math-container">$$ \lim_{n \to \infty} P_n(x) = f(x) $$</span>
    uniformly on <span class="math-container">$[a, b]$</span>. If <span class="math-container">$f$</span> is real, the <span class="math-container">$P_n$</span> may be taken real.</p></blockquote>
  <p><strong>DEFINITION x. (Uniform Convergence of functions) </strong> $\quad$ Let $\{f_n\}_n\geq1$ be a sequence of functions, $f_n:X\to Y$ converges to $f$ uniformly if for all $\epsilon>0$, there exists $N_\epsilon \in \mathbb{N}$ such that for all $n \geq N_\epsilon$, $\lvert f_n(x)-f(x) \rvert < \epsilon$ for all $x \in X$.  </p>
  <p> Uniform convergence is a key concept that ensures the accuracy of using polynomials to approximate arbitrary continuous functions on [a, b]. 
  </p>
  <blockquote><p><strong>Proof:</strong></p>
    <p>We may assume, without loss of generality that <span class="math-container">$[a, b] = [0, 1]$</span>. We may also assume that <span class="math-container">$f(0) = f(1) = 0$</span>. For if the theorem is proved for this case, consider <span class="math-container">$$ g(x) = f(x) - f(0) - x [ f(1) - f(0) ] \qquad (0 \leq x \leq 1). $$</span> Here <span class="math-container">$g(0) = g(1) = 0$</span>, and if <span class="math-container">$g$</span> can be obtained as the limit of a uniformly convergent sequence of polynomials, it is clear that the same is true for <span class="math-container">$f$</span>, since <span class="math-container">$f-g$</span> is a polynomial.</p></blockquote>
  Here is a further clarification on how proving the result on the interval [0, 1] would be sufficient to generalize to an arbitrary interval [a, b]. We define a function  <span class="math-container"> $$ g(x): [0,1] \to \mathbb{C} \\ f(x): [a,b] \to \mathbb{C} \\ g(x) = f((b-a)x+a)  $$ </span> and therefore we can convert it back <span class="math-container"> $$ f(x) = g(\frac{x-a}{b-a})$$</span> We can make a further modification that <span class="math-container"> $$ g(x) = f((b-a)x+a) - ((f(b)-f(a))x+f(a)) $$</span> so that <span class="math-container"> $$ g(0) = g(1) = 0, \\ $$ </span> and since $f-g$ is a polynomial, if we get a sequence of polynomials <span class="math-container"> $ \{P_n\} $ on $[0, 1]$ that converges uniformly to $g$, then rescaling and shifting $\{P_n\}$ with polynomials the same way we get $f$ from $g$ would get us a sequence of polynomials converging uniformly to $\:f.\\$ </span>
  <p><strong>DEFINITION x. (Uniform Continuity)  </strong>  $\quad f: X \to Y$ is <strong>uniformly continuous</strong> if for all $\varepsilon > 0$, there exists $\delta > 0$ such that $\lvert f(x) - f(y) \rvert < \varepsilon $ whenever $\lvert x - y \rvert < \delta$ </p>
  <blockquote><p>Furthermore, we define <span class="math-container">$f(x)$</span> to be zero for <span class="math-container">$x$</span> outside <span class="math-container">$[0, 1]$</span>. Then <span class="math-container">$f$</span> is uniformly continuous on the whole line. </p></blockquote>
  <p><strong>THEOREM x. (Heine-Borel Theorem)</strong> $\quad$ A subspace of $\mathbb{R}^n$ is compact $\iff$ it's closed and bounded.  </p>
    <p><strong>THEOREM x. (Continuous and Compact Domain)</strong> $\quad$ If $X$ is compact and $f:X\to \mathbb{C}$ is continuous, then $f$ is uniformly continuous. </p>
  <p><strong>DEFINITION x. (Compact Support) </strong> $\quad$ A function has compact support if it is zero outside of a compact set. </p>
    <p> We know that [0, 1], [a, b] are compact sets and $f$ is continuous on the intervals, so $f$ has a compact support and is uniformly continuous. We'll see how being compactly supported and the uniform continuity of $f$ becomes useful in the last segment of the proof. </p>
<blockquote><p>We put
<span class="math-container">$$\tag{1}  Q_n(x) = c_n \left( 1- x^2 \right)^n \qquad (n = 1, 2, 3, \ldots), $$</span>
where <span class="math-container">$c_n$</span> is chosen so that
<span class="math-container">$$ \tag{2} \int_{-1}^1 Q_n(x) \ \mathrm{d} x = 1 \qquad (n = 1, 2, 3, \ldots). $$</span>
We need some information about the order of magnitude of <span class="math-container">$c_n$</span>. Since
<span class="math-container">$$ 
\begin{align} 
\int_{-1}^1 \left( 1-x^2 \right)^n \ \mathrm{d} x &amp;= 2 \int_0^1 \left( 1-x^2 \right)^n \ \mathrm{d} x \\
 &amp;\geq 2 \int_0^{1/\sqrt{n}} \left( 1-x^2 \right)^n \ \mathrm{d} x \\
&amp; \geq 2 \int_0^{1/\sqrt{n}} \left( 1- n x^2 \right) \ \mathrm{d} x \\
&amp;= \frac{4}{3 \sqrt{n} } \\
&amp;&gt; \frac{1}{ \sqrt{n} }, 
\end{align}
$$</span>
  it follows from (2) that <span class="math-container">$$ \tag{3} c_n &lt; \sqrt{n}. $$</span>
 The inequality <span class="math-container">$\left( 1-x^2 \right)^n \geq 1-nx^2$</span> which we used above is easily shown to be true by considering the function
<span class="math-container">$$ \left( 1- x^2 \right)^n - 1+nx^2 $$</span>
  which is zero at <span class="math-container">$x= 0$</span> and whose derivative is positive in <span class="math-container">$(0, 1)$. </span></p></blockquote>
  <p>
  You can also expand $\left( 1-x^2 \right)^n$ using binomial theorem 
    $$\begin{align*}
(1 - x^2)^n  &amp;= \sum_{k=0}^{n} (-1)^k \binom{n}{k} x^{2k} \\
&amp;= 1 - nx^2 + \text{higher-order terms,} 
    \end{align*}$$ where the sum of the higher-order terms is nonnegative </p>
<blockquote><p>For any <span class="math-container">$\delta &gt; 0$</span>, (3) implies
<span class="math-container">$$ \tag{4} Q_n(x) \leq \sqrt{n} \left( 1- \delta^2 \right)^n \qquad ( \delta \leq \lvert x \rvert \leq 1), $$</span>
  so that <span class="math-container">$Q_n \to 0$</span> uniformly in <span class="math-container">$\delta \leq \lvert x \rvert \leq 1$</span>. </p></blockquote>
 <p>Not explicitly mentioned but $\delta \in (0, 1)$, which implies $\\(1-\delta^2)^n \to 0$ $ \text{ exponentially, faster than $\sqrt{n}$,} \text{ therefore } \sqrt{n}(1-\delta^2)^n \to 0 $
  </p>
  <blockquote><p>Now set
    <span class="math-container">$$ \tag{5}  P_n(x) = \int_{-1}^1 f(x+t) Q_n (t) \ \mathrm{d} t \qquad (0 \leq x \leq 1). $$</span></p></blockquote>
  Notice that this is actually a convolution.<p>
  <strong>DEFINITION 4. (Convolution) </strong> $\quad$ The convolution of $f$ and $g$ is written $f \ast g$. It is defined as $$(f \ast g)(t):=\int_{-\infty}^{\infty} f(\tau) g(t-\tau) d \tau$$ </p>
  <blockquote><p>
Our assumptions about <span class="math-container">$f$</span> show, by a simple change of variable, that
<span class="math-container">$$ P_n(x) = \int_{-x}^{1-x} f(x+t) Q_n(t) \ \mathrm{d} t = \int_0^1 f(t) Q_n(t-x) \ \mathrm{d} t, $$</span>
    and the last integral is clearly a polynomial in <span class="math-container">$x$</span>. Thus <span class="math-container">$\left\{ P_n \right\}$</span> is a sequence of polynomials, which are real if <span class="math-container">$f$</span> is real.</p></blockquote>
  <p>It may actually not be immediately clear that the last integral is a polynomial in $x$.
  </p>
  <blockquote><p>Given <span class="math-container">$\varepsilon &gt; 0$</span>, we choose <span class="math-container">$\delta &gt; 0$</span> such that <span class="math-container">$\lvert y-x \rvert &lt; \delta$</span> implies <span class="math-container">$$ \lvert f(y) - f(x) \rvert &lt; \frac{\varepsilon}{2}. $$</span></p></blockquote>
  This is by uniform continuity of $f\\$.
  <blockquote><p>
Let <span class="math-container">$M = \sup \lvert f(x) \rvert$</span>. Using (2), (4), and the fact that <span class="math-container">$Q_n(x) \geq 0$</span>, we see that for <span class="math-container">$0 \leq x \leq 1$</span>,
<span class="math-container">$$ 
\begin{align}
&amp; \ \ \  \left\lvert P_n(x) - f(x) \right\rvert \\ 
&amp;= \left\lvert \int_{-1}^1 [ f(x+t) - f(x) ] Q_n(t) \ \mathrm{d} t \right\rvert \\
&amp;\leq \int_{-1}^1 \lvert f(x+t) - f(x) \rvert Q_n(t) \ \mathrm{d} t \\
&amp;\leq 2M \int_{-1}^{-\delta} Q_n(t) \ \mathrm{d} t + \frac{\varepsilon}{2} \int_{-\delta}^\delta Q_n(t) \ \mathrm{d} t + 2 M \int_\delta^1 Q_n(t) \ \mathrm{d} t \\
&amp;\leq 4M \sqrt{n} \left( 1 - \delta^2 \right)^n + \frac{\varepsilon}{2} \\
&amp;&lt; \varepsilon
\end{align}
$$</span>
for all large enough <span class="math-container">$n$</span>, which proves the theorem.
    <span class="d-none">$\endgroup$</span></p></blockquote> 
  <p> 
    $$\left\lvert(f \ast Q_n)(x) - f(x)\right\rvert =\int_{-\infty}^{\infty} f(\tau) g(t-\tau) d \tau$$
    Maximum and minimum is attained for a continuous function on a compact set, so we can upperbound $f$ by $M$ explicitly: $\lvert f(x+t) - f(x) \rvert \leq 2M. \\$<br>
    Then, since $f$ is uniformly continuous, we choose a $\delta > 0$ such that $\lvert f(x_1)-f(x_2)\rvert < \frac{\varepsilon}{2} $, if $\lvert x_1 - x_2 \rvert < \delta\\$.
    Next, by $(4)$, we can combine the first and the third term to see that their sum is upperbounded by $4M \sqrt{n} \left( 1- \delta^2 \right)^n $, and the integral in the second term upperbounded by 1, according to $(2)$. 
  </p> <p>Finally, since $\sqrt{n}(1-\delta^2)^n \to 0 $, $4M\sqrt{n}(1-\delta^2)^n < \frac{\varepsilon}{2}$ as $n \to \infty$. $\\$                                                                               The terms add up to be strictly less than $\varepsilon$, which completes the proof that ${P_n}$ uniformly converges to f.                                                                                                             </p>                                                                                         
</div>

<br>

---



**Cybenko Theorem**

The proof of this theorem involves more background knowledge in functional analysis. If you're interested in the complete proof, I recommend you go to Cybenko's original paper. Here I will give a high level sketch of the proof. 

The version that Cybenko proved is for neural network with one hidden layer and arbitrary witdth. 

Here is the theorem in its original form,



<div class="postcell post-layout--right">  
  <blockquote><strong>Theorem (Cybenko)</strong> $\quad$ Let $\sigma$ be any continuous discriminatory function. 
Then finite sums of the form
$$
G(x) = \sum_{j=1}^{N} \alpha_j \sigma(w_j^T x + b_j),
$$ 
where $w_j \in \mathbb{R}^n$, $\alpha_j, b_j \in \mathbb{R}$, are dense in $C(I_n)$.

<p>In other words, given any $\varepsilon > 0$ and $f \in C(I_n)$, there is a sum $G(x)$ of the above form such that</p>
$$
|G(x) - f(x)| < \varepsilon, \quad \forall x \in I_n.
                            $$</blockquote>
    <p>Here, the neural network $G:\mathbb{R}^n \to \mathbb{R}$ has $N$ neurons in the single hidden layer, given input $x \in \mathbb{R}^n$. Each $w_j$ corresponds to weights between the input $x$ and each of the hidden neurons, $b_j$ is a bias term, and $a_j$ is the weight between the $j$ th neuron and the output. $\sigma$ is a continuous <strong>$sigmoidal$</strong> function.
</p>
<blockquote><strong>Definition x. (Sigmoidal function)</strong> $\quad$ We say that $\sigma$ is $sigmoidal$ if
$$
\sigma(t) \rightarrow \left\{
\begin{array}{ll}
      1 & \text{as } t \rightarrow +\infty, \\
      0 & \text{as } t \rightarrow -\infty. \\
\end{array} 
\right.
  $$</blockquote>
 <p>
  The theorem is obtained through combining the two results he has proven. 
  </p>
  <strong>Definition (discriminatory)</strong> $\quad$ We say that a function $\sigma$ is discriminatory if, given a measure $\mu \in M(I_n)$ such that
$$
\int_{I_n} \sigma(w^T x + b) \, d\mu(x) = 0, \quad \forall w \in \mathbb{R}^n, b \in \mathbb{R}
$$
<p>implies that $\mu = 0$.</p>
</div>






<div class="postcell post-layout--right">  
 <p>
haha
  </p>                                                                                         
</div>






Hornik Theorem

Universal Approximation Theorem for Deep Learning

---

There are still some situations where the neural activities are not best described by continuous functions. I believe the first example that pop up in many people's mind is the generation of action potentials, where spike events are modeled by dirac-delta functions. 



---

Reference:

*Marr, David, Vision: A Computational Investigation into the Human Representation and Processing of Visual Information (Cambridge, MA, 2010; online edn, MIT Press Scholarship Online, 22 Aug. 2013), https://doi.org/10.7551/mitpress/9780262514620.001.0001, accessed 19 Dec. 2021.*

Image credit

[Ellen Lupton](https://inosensiasharenagathahome.files.wordpress.com/2020/01/marrimg2.gif?w=594)



https://math.stackexchange.com/questions/2507841/theorem-7-26-in-baby-rudin-the-stone-weierstrass-theorem

https://www.youtube.com/watch?v=HvR6ZhqojAc&ab_channel=ThatMathThing

https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf

https://bondmatt.files.wordpress.com/2009/06/weierstrass.pdf connection to convolutions and approximate identity
