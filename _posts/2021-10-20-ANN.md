---
layout: post
title: Why are neural networks "universal approximators"?
subtitle: The Universal Approximation Theorem
cover-img: /assets/img/process1.jpg
thumbnail-img: /assets/img/vision.png
share-img: /assets/img/path.jpg
tags: 
- Machine Learning
- Math
---



A gentle start,

**Theorem 1 (Weierstrass Approximation Theorem).** Let $f:[a,b] \rightarrow\mathbb{R}$ be a continuous function, then there exists a polynomial $p$ on $[a,b]$ such that for any $\epsilon>0$, 



Now let's first simplify this problem a little bit, by restricting it to the case when $f \in C[0,1]$



**Definition 1 (Uniform Convergence).**    A sequence of functions {$f_n$}, $n=1,2,3...,$ converges uniformly on $E$ to

**Definition 2 (Uniform Continuity).**

If a function f on a compact set is continuous it's 

$Q_n(x)=c_n(1-x^2)^n$

compactly supported

Convolution



Let's take a look at the proof in Baby Rudin(Principles of Mathematical Analysis, by Walter Rudin)



<div style="text-align:center">  $$ \begin{align*} 
  \tag{1} Q_n(x)=c_n(1-x^2)^n \hspace{1cm} (n=1,2,3...), \\
  \end{align*} $$
</div>

where $c_n$ is chosen so that

<div style="text-align:center">  $$ \begin{align*} 
  \tag{2} \int_{1}^{-1} Q_n(x) \,dx=1 \hspace{1cm}   (n=1,2,3...),\\
  \end{align*} $$
</div>





<div style="text-align:center"> 
  <span class="math-container">$$ 
\begin{align} 
\int_{-1}^1 \left( 1-x^2 \right)^n \ \mathrm{d} x &amp;= 2 \int_0^1 \left( 1-x^2 \right)^n \ \mathrm{d} x \\
 &amp;\geq 2 \int_0^{1/\sqrt{n}} \left( 1-x^2 \right)^n \ \mathrm{d} x \\
&amp;\geq 2 \int_0^{1/\sqrt{n}} \left( 1- n x^2 \right) \ \mathrm{d} x \\
&amp;= \frac{4}{3 \sqrt{n} } \\
&amp;&gt; \frac{1}{ \sqrt{n} } \\
    \sqrt{n} \int_{-1}^1 \left( 1-x^2 \right)^n \ \mathrm{d} x &amp;> 1 \\
\end{align}
$$</span>
</div>

it follows from $(2)$ that

<div style="text-align:center">  $$ \begin{align*} 
  \tag{3} c_n &amp;< \sqrt{n}
   \end{align*} $$
</div>

For any $\delta >0,$ $(3)$ implies



<div style="text-align:center">  $$ \begin{align*} 
  \tag{4} Q_n(x) \leq \sqrt{n}(1-\delta^2)^n
   \end{align*} $$
</div>

so that $Q_n \to 0$ uniformly in $\delta \leq \lvert x \rvert \leq 1$



<div class="postcell post-layout--right">
    <span class="d-none">$\begingroup$</span>
    <div class="s-prose js-post-body" itemprop="text">
<p>Now set
<span class="math-container">$$ \tag{5}  P_n(x) = \int_{-1}^1 f(x+t) Q_n (t) \ \mathrm{d} t \qquad (0 \leq x \leq 1). $$</span>
Our assumptions about <span class="math-container">$f$</span> show, by a simple change of variable, that
<span class="math-container">$$ P_n(x) = \int_{-x}^{1-x} f(x+t) Q_n(t) \ \mathrm{d} t = \int_0^1 f(t) Q_n(t-x) \ \mathrm{d} t, $$</span>
and the last integral is clearly a polynomial in <span class="math-container">$x$</span>. Thus <span class="math-container">$\left\{ P_n \right\}$</span> is a sequence of polynomials, which are real if <span class="math-container">$f$</span> is real.</p>
<p>Given <span class="math-container">$\varepsilon &gt; 0$</span>, we choose <span class="math-container">$\delta &gt; 0$</span> such that <span class="math-container">$\lvert y-x \rvert &lt; \delta$</span> implies <span class="math-container">$$ \lvert f(y) - f(x) \rvert &lt; \frac{\varepsilon}{2}. $$</span>
Let <span class="math-container">$M = \sup \lvert f(x) \rvert$</span>. Using (2), (4), and the fact that <span class="math-container">$Q_n(x) \geq 0$</span>, we see that for <span class="math-container">$0 \leq x \leq 1$</span>,
<span class="math-container">$$ 
\begin{align}  \left\lvert P_n(x) - f(x) \right\rvert  
&amp;= \left\lvert \int_{-1}^1 [ f(x+t) - f(x) ] Q_n(t) \ \mathrm{d} t \right\rvert \\
&amp;\leq \int_{-1}^1 \lvert f(x+t) - f(x) \rvert Q_n(t) \ \mathrm{d} t \\
&amp;\leq 2M \int_{-1}^{-\delta} Q_n(t) \ \mathrm{d} t + \frac{\varepsilon}{2} \int_{-\delta}^\delta Q_n(t) \ \mathrm{d} t + 2 M \int_\delta^1 Q_n(t) \ \mathrm{d} t \\
&amp;\leq 4M \sqrt{n} \left( 1 - \delta^2 \right)^n + \frac{\varepsilon}{2} \\
&amp;&lt; \varepsilon
\end{align}
$$</span>
</p>
  </div>

</div>

for all large nough $n$, which proves the theorem.

<div style="text-align:center">  $$ \begin{align*} 
  c_n < \sqrt{n} \\
  \end{align*} $$
</div>

Therefore, 



<div style="text-align:center">  $$ \begin{align*} 
  c_n < \sqrt{n} \\
  \end{align*} $$
</div>



**Theorem 2 (Stone-Weierstrass Theorem).** 

Cybenko Theorem

Hornik TheoremUniversal Approximation Theorem for Deep Learning



---

*Marr, David, Vision: A Computational Investigation into the Human Representation and Processing of Visual Information (Cambridge, MA, 2010; online edn, MIT Press Scholarship Online, 22 Aug. 2013), https://doi.org/10.7551/mitpress/9780262514620.001.0001, accessed 19 Dec. 2021.*

Image credit

[Ellen Lupton](https://inosensiasharenagathahome.files.wordpress.com/2020/01/marrimg2.gif?w=594)

